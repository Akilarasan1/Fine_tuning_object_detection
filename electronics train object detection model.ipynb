{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adee98e9-acd1-4717-90da-31ec24eee4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.digitalocean.com/community/tutorials/few-shot-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90aeb26d-5087-4134-8ec7-109b08d8ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = r\"C:\\Users\\akilarasan.p\\Downloads\\archive\\samples_for_clients\\samples_for_clients\"\n",
    "annotations_dir = r\"C:\\Users\\akilarasan.p\\Downloads\\archive\\annotations\\annotations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bef5300-4023-41dd-b0d3-4bf0da26f41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akilarasan.p\\.conda\\envs\\jup_ocr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.8 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from xml.etree.ElementTree import parse\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2 as ToTensor\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir, annotations_dir, transforms=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.imgs = list(sorted(os.listdir(images_dir)))\n",
    "        self.mask_labels = ['keyboard', 'monitor', 'mouse', 'laptop', 'mobile']  \n",
    "\n",
    "        bbox_params = A.BboxParams(format='albumentations', label_fields=['class_labels'])\n",
    "        self.transforms = transforms or A.Compose([\n",
    "            A.HorizontalFlip(p=0.1),\n",
    "            A.VerticalFlip(p=0.1),\n",
    "            A.RandomBrightnessContrast(p=0.1),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), \n",
    "            ToTensor()], bbox_params=bbox_params)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.images_dir, self.imgs[idx])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        height, width, _ = image.shape\n",
    "\n",
    "        # Parse XML annotation\n",
    "        anno_path = os.path.join(self.annotations_dir, os.path.splitext(self.imgs[idx])[0] + '.xml')\n",
    "        parser = parse(anno_path)\n",
    "        labels, boxes = [], []\n",
    "\n",
    "        for obj in parser.findall('object'):\n",
    "            bndbox = obj.find('bndbox')\n",
    "            if bndbox.find('xmin').text != bndbox.find('xmax').text:\n",
    "                box = [float(bndbox.find(c).text) for c in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
    "                # Normalize bounding boxes\n",
    "                box = [\n",
    "                    max(0.0, min(box[0] / width, 1.0)),   # xmin\n",
    "                    max(0.0, min(box[1] / height, 1.0)),  # ymin\n",
    "                    max(0.0, min(box[2] / width, 1.0)),   # xmax\n",
    "                    max(0.0, min(box[3] / height, 1.0))   # ymax\n",
    "                ]\n",
    "                label = obj.find('name').text\n",
    "                label = self.mask_labels.index(label)\n",
    "                boxes.append(box)\n",
    "                labels.append(label)\n",
    "\n",
    "        # Handle empty annotations\n",
    "        if not boxes:\n",
    "            boxes = [(0.0, 0.0, 1.0, 1.0)]\n",
    "            labels = [0]\n",
    "\n",
    "        # Apply transforms\n",
    "        transformed = self.transforms(image=image, bboxes=boxes, class_labels=labels)\n",
    "        image = transformed['image'].float()  # Ensure float32 type\n",
    "        boxes = torch.tensor(transformed['bboxes'], dtype=torch.float32)\n",
    "        labels = torch.tensor(transformed['class_labels'], dtype=torch.int64)\n",
    "\n",
    "        # Additional target metadata\n",
    "        area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "        image_id = torch.tensor([idx])\n",
    "        iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n",
    "\n",
    "        # Construct target dictionary\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': image_id,\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "images_dir = r\"C:\\Users\\akilarasan.p\\Downloads\\archive\\samples_for_clients\\samples_for_clients\"\n",
    "annotations_dir = r\"C:\\Users\\akilarasan.p\\Downloads\\archive\\annotations\\annotations\"\n",
    "\n",
    "dataset = CustomDataset(images_dir, annotations_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8241fc41-fec9-43e7-b90b-6b6da0d497c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in c:\\users\\akilarasan.p\\.conda\\envs\\jup_ocr\\lib\\site-packages (1.4.10)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting albumentations\n",
      "  Downloading albumentations-2.0.8-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\akilarasan.p\\.conda\\envs\\jup_ocr\\lib\\site-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\akilarasan.p\\.conda\\envs\\jup_ocr\\lib\\site-packages (from albumentations) (1.15.2)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\akilarasan.p\\.conda\\envs\\jup_ocr\\lib\\site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in c:\\users\\akilarasan.p\\.conda\\envs\\jup_ocr\\lib\\site-packages (from albumentations) (2.11.2)\n",
      "Collecting albucore==0.0.24 (from albumentations)\n",
      "  Using cached albucore-0.0.24-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\users\\akilarasan.p\\.conda\\envs\\jup_ocr\\lib\\site-packages (from albumentations) (4.11.0.86)\n",
      "Collecting stringzilla>=3.10.4 (from albucore==0.0.24->albumentations)\n",
      "  Downloading stringzilla-3.12.5-cp310-cp310-win_amd64.whl.metadata (81 kB)\n",
      "Collecting simsimd>=5.9.2 (from albucore==0.0.24->albumentations)\n",
      "  Downloading simsimd-6.4.9-cp310-cp310-win_amd64.whl.metadata (67 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\akilarasan.p\\.conda\\envs\\jup_ocr\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\akilarasan.p\\.conda\\envs\\jup_ocr\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\akilarasan.p\\.conda\\envs\\jup_ocr\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\akilarasan.p\\.conda\\envs\\jup_ocr\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (0.4.0)\n",
      "Downloading albumentations-2.0.8-py3-none-any.whl (369 kB)\n",
      "Using cached albucore-0.0.24-py3-none-any.whl (15 kB)\n",
      "Downloading simsimd-6.4.9-cp310-cp310-win_amd64.whl (94 kB)\n",
      "Downloading stringzilla-3.12.5-cp310-cp310-win_amd64.whl (80 kB)\n",
      "Installing collected packages: stringzilla, simsimd, albucore, albumentations\n",
      "  Attempting uninstall: albucore\n",
      "    Found existing installation: albucore 0.0.13\n",
      "    Uninstalling albucore-0.0.13:\n",
      "      Successfully uninstalled albucore-0.0.13\n",
      "  Attempting uninstall: albumentations\n",
      "    Found existing installation: albumentations 1.4.10\n",
      "    Uninstalling albumentations-1.4.10:\n",
      "      Successfully uninstalled albumentations-1.4.10\n",
      "Successfully installed albucore-0.0.24 albumentations-2.0.8 simsimd-6.4.9 stringzilla-3.12.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "paddleocr 2.9.1 requires albucore==0.0.13, but you have albucore 0.0.24 which is incompatible.\n",
      "paddleocr 2.9.1 requires albumentations==1.4.10, but you have albumentations 2.0.8 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# pip install --upgrade albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33a9d2b-d51c-42a0-9dfa-5d817861e561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akilarasan.p\\.conda\\envs\\jup_ocr\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\akilarasan.p\\.conda\\envs\\jup_ocr\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def get_model(num_classes):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "num_classes = 5\n",
    "# ['keyboard', 'monitor', 'mouse', 'laptop', 'mobile']\n",
    "\n",
    "model = get_model(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ce78e3-0945-4e99-995e-f77ea84eb5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.accelerator.current_accelerator().type if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe7e39-4cdf-4ae4-8527-a3928f040782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/5\n",
      "Batch 1 Loss: 3.9589\n",
      "Batch 2 Loss: 1.0440\n",
      "Batch 3 Loss: 0.4855\n",
      "Batch 4 Loss: 0.6695\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=5, shuffle=True, collate_fn=collate_fn)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "save_path = \"model_epoch_{}.pth\"\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0  # Track loss for the epoch\n",
    "    for batch_idx, (images, targets) in enumerate(data_loader, 1):\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "    \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Batch {batch_idx} Loss: {losses.item():.4f}\")\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    avg_loss = running_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch + 1} Completed. Average Loss = {avg_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), save_path.format(epoch + 1))\n",
    "    print(f\"Model saved at {save_path.format(epoch + 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce28e01c-f0fe-41d6-b3f8-0356e1cd008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml file read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19311251-c687-4d62-be48-4474604874ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "annotations_path = r'C:\\Users\\akilarasan.p\\Downloads\\archive\\annotations\\annotations'\n",
    "data = []\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(annotations_path):\n",
    "    if filename.endswith('.xml'):\n",
    "        file_path = os.path.join(annotations_path, filename)\n",
    "        \n",
    "        # Read the XML file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            xml_content = file.read()\n",
    "        \n",
    "        # Parse the XML with BeautifulSoup\n",
    "        soup = BeautifulSoup(xml_content, 'xml')\n",
    "        \n",
    "        # Find all object tags\n",
    "        objects = soup.find_all('object')\n",
    "        \n",
    "        # Loop through objects and get their names\n",
    "        object_names = [obj.find('name').text for obj in objects]\n",
    "        \n",
    "        # Print the object names for each file\n",
    "        # print(f\"File: {filename}\")\n",
    "        # print(\"Object names:\", object_names)\n",
    "        data.append(' '.join(object_names))\n",
    "        # print(\"-\" * 40)\n",
    "\n",
    "\n",
    "list(set(' '.join(data).split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436ca64-c80c-403a-86e8-5675218d09ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
