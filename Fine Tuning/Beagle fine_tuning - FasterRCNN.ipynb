{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291a5f58-b524-4c2d-a57a-19f1d4e6d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r'C:\\Users\\akilarasan.p\\Downloads\\fine-tune-MaskRcnn-master\\beagle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f006f37-4e09-400d-90c7-df67a487ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f50fd5ff-6e79-44fd-8b5f-98456bbbacdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeagleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir , transforms = None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        #load the annotations file, it also contain information of image names\n",
    "        # load the annotations\n",
    "        annotations1 = json.load(open(os.path.join(data_dir, 'via_region_data.json')))\n",
    "        self.annotations = list(annotations1.values())\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        #get the image path from the annotations data\n",
    "        img_name = self.annotations[idx]['filename']\n",
    "        img_path = os.path.join(self.data_dir,img_name)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        #first id is the background , objects count from 1\n",
    "        obj_ids = np.array(range(len(self.annotations[idx]['regions']))) + 1\n",
    "\n",
    "        #get bounding box coordinates for each objects\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "\n",
    "        for i in range(num_objs):\n",
    "            xmin = np.min(self.annotations[idx][\"regions\"][i][\"shape_attributes\"][\"all_points_x\"])\n",
    "            xmax = np.max(self.annotations[idx][\"regions\"][i][\"shape_attributes\"][\"all_points_x\"])\n",
    "            ymin = np.min(self.annotations[idx][\"regions\"][i][\"shape_attributes\"][\"all_points_y\"])\n",
    "            ymax = np.max(self.annotations[idx][\"regions\"][i][\"shape_attributes\"][\"all_points_y\"])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype = torch.float32)\n",
    "\n",
    "        labels = torch.ones((num_objs,),dtype = torch.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0])\n",
    "\n",
    "        iscrowd = torch.zeros((num_objs, ), dtype = torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = image_id\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b4e3c6-798d-4d55-a5ef-eecedb78a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def build_model(num_classses):\n",
    "    #load the instance segmentation\n",
    "\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n",
    "\n",
    "    #get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d57e2-7508-44c5-bbe2-143099c0283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BeagleDataset(r'C:\\Users\\akilarasan.p\\Downloads\\fine-tune-MaskRcnn-master\\beagle\\train', get_transform(train= True))\n",
    "dataset_test = BeagleDataset(r'C:\\Users\\akilarasan.p\\Downloads\\fine-tune-MaskRcnn-master\\beagle\\val', get_transform(train= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de986309-f4c6-456b-a5fd-1fbd9b8dc49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = BeagleDataset(r'C:\\Users\\akilarasan.p\\Downloads\\fine-tune-MaskRcnn-master\\beagle\\train', get_transform(train=True))\n",
    "dataset_test = BeagleDataset(r'C:\\Users\\akilarasan.p\\Downloads\\fine-tune-MaskRcnn-master\\beagle\\val', get_transform(train=False))\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ae20cf9-ec83-4215-95d2-0a9db4a6cbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "015951ee-113b-46cb-ada6-66ba8540e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# our dataset has two classes only - background and beagle\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = build_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4287633-c09c-4c88-b331-502bdc00f2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/38]  eta: 0:15:46  lr: 0.000140  loss: 0.1158 (0.1158)  loss_classifier: 0.0314 (0.0314)  loss_box_reg: 0.0621 (0.0621)  loss_objectness: 0.0004 (0.0004)  loss_rpn_box_reg: 0.0219 (0.0219)  time: 24.9039  data: 0.1146  max mem: 0\n",
      "Epoch: [0]  [10/38]  eta: 0:11:58  lr: 0.001490  loss: 0.1240 (0.1323)  loss_classifier: 0.0243 (0.0257)  loss_box_reg: 0.0884 (0.0882)  loss_objectness: 0.0034 (0.0043)  loss_rpn_box_reg: 0.0118 (0.0140)  time: 25.6521  data: 0.0712  max mem: 0\n",
      "Epoch: [0]  [20/38]  eta: 0:07:53  lr: 0.002840  loss: 0.1288 (0.1332)  loss_classifier: 0.0243 (0.0269)  loss_box_reg: 0.0897 (0.0916)  loss_objectness: 0.0027 (0.0047)  loss_rpn_box_reg: 0.0054 (0.0100)  time: 26.4038  data: 0.0631  max mem: 0\n",
      "Epoch: [0]  [30/38]  eta: 0:03:31  lr: 0.004190  loss: 0.1114 (0.1249)  loss_classifier: 0.0222 (0.0259)  loss_box_reg: 0.0744 (0.0834)  loss_objectness: 0.0011 (0.0041)  loss_rpn_box_reg: 0.0083 (0.0115)  time: 26.9553  data: 0.0559  max mem: 0\n",
      "Epoch: [0]  [37/38]  eta: 0:00:26  lr: 0.005000  loss: 0.0999 (0.1196)  loss_classifier: 0.0199 (0.0244)  loss_box_reg: 0.0670 (0.0791)  loss_objectness: 0.0008 (0.0036)  loss_rpn_box_reg: 0.0089 (0.0125)  time: 25.8086  data: 0.0555  max mem: 0\n",
      "Epoch: [0] Total time: 0:16:28 (26.0097 s / it)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# evaluate on the test dataset\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\jup_ocr\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\LEARNING_WORK\\Fine Tuning\\engine.py:86\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, data_loader, device)\u001b[0m\n\u001b[0;32m     83\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m image)\n\u001b[0;32m     84\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m---> 86\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m model_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     88\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(image)\n",
      "File \u001b[1;32m~\\.conda\\envs\\jup_ocr\\lib\\site-packages\\torch\\cuda\\__init__.py:790\u001b[0m, in \u001b[0;36msynchronize\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msynchronize\u001b[39m(device: _device_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    783\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for all kernels in all streams on a CUDA device to complete.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m \n\u001b[0;32m    785\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;124;03m            if :attr:`device` is ``None`` (default).\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m    792\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_synchronize()\n",
      "File \u001b[1;32m~\\.conda\\envs\\jup_ocr\\lib\\site-packages\\torch\\cuda\\__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# number of epochs\n",
    "num_epochs = 10\n",
    " \n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54110146-8dd9-4ca5-bad3-e0e3440c7a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to evaluation mode\n",
    "torch.save(model, 'faster-rcnn-beagle.pt')\n",
    "model.eval()\n",
    "CLASS_NAMES = ['__background__', 'beagle']\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
